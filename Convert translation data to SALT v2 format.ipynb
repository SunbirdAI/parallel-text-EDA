{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84a843c-72dd-4fd1-9bc5-595fd2835b56",
   "metadata": {},
   "source": [
    "# Conversion of machine translation datasets to SALT v2 format\n",
    "\n",
    "This notebook converts existing machine translation datasets that we have used, including the first version of SALT plus third party datasets, and converts it into the v2 format.\n",
    "\n",
    "The new format is .jsonl and looks like this:\n",
    "\n",
    "    [\n",
    "    {'text' : {\n",
    "        'lug' : 'Oli otya?,\n",
    "        'ach' : 'Itye maber?'\n",
    "        'eng' : 'How are you?'}}\n",
    "    {'text' : {\n",
    "        'lug' : 'Weebale,\n",
    "        'ach' : 'Apwoyo',\n",
    "        'eng' : 'Thank you'}}\n",
    "    ...\n",
    "    ]\n",
    "    \n",
    " The resulting files are compressed and stored in `s3://sunbird-translate`, downloadable [here](https://sunbird-translate.s3.us-east-2.amazonaws.com/salt-translation-plus-external-datasets.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ebadc-8089-474a-9b74-2fd038cf0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import glob\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c667bb-19bc-4f89-9ce7-828f32d1ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '../datasets/salt-translation-plus-external-datasets-15-3-23/'\n",
    "!mkdir -p {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baad3b5-8de8-48cf-a1f8-4773b7edf3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_list(path):\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "        return lines\n",
    "    \n",
    "def url_to_list(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b01177-5d55-41e7-bcd8-2a9018fde6e3",
   "metadata": {},
   "source": [
    "# MT560 datasets\n",
    "\n",
    "MT560 data was already extracted for four languages [here](https://github.com/SunbirdAI/parallel-text-EDA/blob/main/Prepare_supplementary_translation_data_(MT560%2BFLORES101%2BAI4D).ipynb).\n",
    "\n",
    "There are several other African languages not yet processed which can be added. We convert from our previous data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782f5c0-ca19-46db-bae9-04a8de256d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['lug', 'ach', 'nyn', 'luo']\n",
    "\n",
    "DATA_DIR = 'v7-dataset/v7.0/supervised/'\n",
    "\n",
    "if not os.path.exists('v7-dataset'):\n",
    "    !wget https://sunbird-translate.s3.us-east-2.amazonaws.com/v7-dataset.zip\n",
    "    !unzip v7-dataset.zip\n",
    "    display.clear_output()\n",
    "    \n",
    "for language in languages:\n",
    "    source = file_to_list(DATA_DIR + f'mul-en/train_mt560_{language}.src')\n",
    "    target = file_to_list(DATA_DIR + f'mul-en/train_mt560_{language}.tgt')\n",
    "\n",
    "    sentences = []\n",
    "    for s, t in zip(source, target):\n",
    "        sentences.append({'text': {language: s, 'eng': t}})\n",
    "\n",
    "    with open(OUTPUT_DIR + f'mt560_{language}.jsonl', 'w') as outfile:\n",
    "        for entry in sentences:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51821f3d-4f2f-42fa-a505-f124f4b2540c",
   "metadata": {},
   "source": [
    "# Makerere AI4D (Luganda to English)\n",
    "\n",
    "15,000 good quality Luganda to English translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f187554-c9cb-4e1d-af96-94514e028a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lug = file_to_list(DATA_DIR + 'mul-en/train_ai4d.src')\n",
    "en = file_to_list(DATA_DIR + 'mul-en/train_ai4d.tgt')\n",
    "\n",
    "sentences = []\n",
    "for s, t in zip(lug, en):\n",
    "    sentences.append({'text': {'lug': s, 'eng': t}})\n",
    "\n",
    "with open(OUTPUT_DIR + 'ai4d.jsonl', 'w') as outfile:\n",
    "    for entry in sentences:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d4094-fcad-425d-89d0-d2b4ce220285",
   "metadata": {},
   "source": [
    "# FLORES 200\n",
    "\n",
    "This dataset contains 2000 sentences with translations in 44 different African languages. We combine the dev and devtest splits into a single set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd769c-0c38-4915-afa3-b513b488a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('flores200_dataset'):\n",
    "    !wget --trust-server-names https://tinyurl.com/flores200dataset\n",
    "    !tar xvzf flores200_dataset.tar.gz \n",
    "    display.clear_output()\n",
    "\n",
    "languages = ['lug', 'eng', 'ibo', 'ewe', 'fon', 'hau', 'kam', 'kea', 'kik', 'kin',\n",
    "             'kmb', 'kon', 'lin', 'lua', 'luo', 'nso', 'nya', 'gaz', 'run', 'sag',\n",
    "             'sna', 'som', 'sot', 'ssw', 'swh', 'tir', 'tsn', 'tso', 'tum', 'twi',\n",
    "             'umb', 'wol', 'xho', 'zul', 'aka', 'amh', 'aka', 'bam', 'bem', 'cjk',\n",
    "             'dik', 'dyu', 'fuv', 'kbp']\n",
    "\n",
    "\n",
    "source_sentences = {}\n",
    "\n",
    "for language in languages:\n",
    "    dev_path = glob.glob(f'flores200_dataset/dev/{language}*.dev')[0]\n",
    "    devtest_path = glob.glob(f'flores200_dataset/devtest/{language}*.devtest')[0]\n",
    "    source_sentences[language] = file_to_list(dev_path) + file_to_list(devtest_path)\n",
    "    if not len(source_sentences[language]):\n",
    "        raise ValueError(f'No text found for language {language}.')  \n",
    "\n",
    "N = len(source_sentences['lug'])\n",
    "sentences = []\n",
    "for i in range(N):\n",
    "    sentence = {'text': {}}\n",
    "    for language in languages:\n",
    "        sentence['text'][language] = source_sentences[language][i] \n",
    "    sentences.append(sentence)\n",
    "\n",
    "with open(OUTPUT_DIR + f'flores200.jsonl', 'w') as outfile:\n",
    "    for entry in sentences:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61924aa-8204-4f95-bd7f-c789c1c2e36c",
   "metadata": {},
   "source": [
    "# SALT v1 dataset\n",
    "\n",
    "From the entire v1 dataset we create train, dev and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f584ce7-9d2b-47a5-bf8c-a38a623de3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SALT_URL = 'https://github.com/SunbirdAI/salt/blob/main/sunbird-ug-lang-v1.0.jsonl?raw=true'\n",
    "response = requests.get(SALT_URL)\n",
    "result = [json.loads(jline) for jline in response.text.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a549dfb-fbd3-44e8-a3d1-3f821ac0036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_english_text = set()\n",
    "sentences = []\n",
    "for item in result:\n",
    "    if item['English'] not in unique_english_text:\n",
    "        sentence = {'text': {}}\n",
    "        sentence['text']['eng'] = item['English']\n",
    "        sentence['text']['lug'] = item['Luganda']\n",
    "        sentence['text']['ach'] = item['Acholi']\n",
    "        sentence['text']['teo'] = item['Ateso']\n",
    "        sentence['text']['lgg'] = item['Lugbara']\n",
    "        sentence['text']['nyn'] = item['Runyankole']\n",
    "        sentences.append(sentence)\n",
    "        unique_english_text.add(item['English'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9072a3f-014c-481d-93d7-0d85485809ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "order = np.random.permutation(np.arange(len(sentences)))\n",
    "salt_dev = [sentences[i] for i in order[:500]]\n",
    "salt_test = [sentences[i] for i in order[500:1000]]\n",
    "salt_train = [sentences[i] for i in order[1000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81ed8b-d52a-4337-a816-26b154f9f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_dev = set([s['text']['eng'] for s in salt_dev])\n",
    "eng_test = set([s['text']['eng'] for s in salt_test])\n",
    "eng_train = set([s['text']['eng'] for s in salt_train])\n",
    "\n",
    "if eng_dev.intersection(eng_test):\n",
    "    raise ValueError('Overlap between dev and test')\n",
    "if eng_dev.intersection(eng_train):\n",
    "    raise ValueError('Overlap between dev and train')\n",
    "if eng_train.intersection(eng_test):\n",
    "    raise ValueError('Overlap between test and train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f1c2f-051c-4088-9c45-610a7fc19815",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR + f'salt-train.jsonl', 'w') as outfile:\n",
    "    for entry in salt_train:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "with open(OUTPUT_DIR + f'salt-dev.jsonl', 'w') as outfile:\n",
    "    for entry in salt_dev:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "with open(OUTPUT_DIR + f'salt-test.jsonl', 'w') as outfile:\n",
    "    for entry in salt_test:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d41a8-8cf0-439a-9596-a2472c1367bf",
   "metadata": {},
   "source": [
    "# Monolingual text (web scraped)\n",
    "\n",
    "Data was scraped from the web using [this code](https://github.com/SunbirdAI/parallel-text-EDA/tree/main/back_translation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994c025-b265-4fea-af9c-f3dc85d49463",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_prefix = ('https://raw.githubusercontent.com/SunbirdAI/'\n",
    "              'parallel-text-EDA/main/back_translation/data/')\n",
    "english_sentences = url_to_list(url_prefix + 'eng/daily-monitor.txt')\n",
    "english_sentences += url_to_list(url_prefix + 'eng/new-vision.txt')\n",
    "english_sentences = [{'text': {'eng': s}} for s in english_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d45445-ae71-4550-89b6-8c11c6502a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "luganda_sentences = url_to_list(url_prefix + 'lug/bukedde.txt')\n",
    "luganda_sentences += url_to_list(url_prefix + 'lug/makerere.txt')\n",
    "luganda_sentences = [{'text': {'lug': s}} for s in luganda_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06fc69-2323-4b9a-82e1-703d0950443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acholi_sentences = url_to_list(url_prefix + 'ach/acholi-online.txt')\n",
    "acholi_sentences += url_to_list(url_prefix + 'ach/misc.txt')\n",
    "acholi_sentences += url_to_list(url_prefix + 'ach/rupiny.txt')\n",
    "acholi_sentences = [{'text': {'ach': s}} for s in acholi_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d96b2-971a-49f9-bb12-ec1b3b0743fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(acholi_sentences), len(luganda_sentences), len(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b7fff-ab48-4af6-8dd6-71f8a48cb368",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR + f'monolingual-eng.jsonl', 'w') as outfile:\n",
    "    for entry in english_sentences:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "with open(OUTPUT_DIR + f'monolingual-lug.jsonl', 'w') as outfile:\n",
    "    for entry in luganda_sentences:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "with open(OUTPUT_DIR + f'monolingual-ach.jsonl', 'w') as outfile:\n",
    "    for entry in acholi_sentences:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
