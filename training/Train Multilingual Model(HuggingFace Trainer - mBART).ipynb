{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Training notebook for the SALT dataset using a pretrained mBART50 model. \n","\n","### Training Steps:\n","* Loading libraries\n","* Loading dataset files\n","* Preprocessing, tokenizing and adding source language tokens \n","* Loading model \n","* Training\n","* Saving Results"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from IPython import display\n","!pip install transformers\n","!pip install sacrebleu\n","!pip install sacremoses\n","!pip install datasets\n","!pip install wandb\n","!pip install sentencepiece\n","display.clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import datasets\n","from IPython import display\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","import sentencepiece\n","import sacrebleu\n","import sacremoses\n","import tqdm\n","import transformers\n","import torch\n","import wandb\n","\n","from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, Seq2SeqTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.mkdir(\"/kaggle/temp\")\n","os.chdir(\"/kaggle/temp\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Using a config dictionary to store hyperparameters and arguments is useful, as it can be passed on to wandb along with the performance. Allows for ease of replication.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Parameters for mul-en models\n","config = {\n","    'source_languages': [\"ach\", \"lgg\", \"lug\", \"nyn\", \"teo\"],\n","    'target_languages': ['en'],\n","    'metric_for_best_model': 'loss',\n","    'train_batch_size': 1,\n","    'gradient_accumulation_steps': 2400,\n","    'max_input_length': 128,\n","    'max_target_length': 128,\n","    'validation_samples_per_language': 500,\n","    'validation_train_merge': True,\n","    'eval_batch_size': 1,\n","    'eval_languages': [\"ach\", \"lgg\", \"lug\", \"nyn\", \"teo\"],\n","    'eval_pretrained_model': False,\n","    'learning_rate': 1e-4,\n","    'num_train_epochs': 2,\n","    'label_smoothing_factor': 0.1,\n","    'flores101_training_data': True,\n","    'mt560_training_data': True,\n","    'back_translation_training_data': False,\n","    'front_translation_training_data': False, #not implemented\n","    'named_entities_training_data': False,\n","    'recycle_language_tokens': True\n","}\n","\n","config['language_pair'] = f'salt-en'\n","config['wandb_project'] = f'salt-mbart'\n","config['wandb_entity'] = f'sunbird'\n","\n","config['model_checkpoint'] = f'facebook/mbart-large-50'\n","\n","# What training data to use\n","config['training_extra_data_dir'] = f'v7-dataset/v7.0/supervised/mul-en/'\n","\n","# Evaluate roughly every 10 minutes\n","eval_steps_interval = 350 * 60 * 7 / (config['gradient_accumulation_steps']\n","                                      * config['train_batch_size'])\n","\n","eval_steps_interval = 4 * max(1, int(eval_steps_interval / 10))\n","\n","print(f'Evaluating every {eval_steps_interval} training steps.')\n","\n","config['train_settings'] = transformers.Seq2SeqTrainingArguments(\n","    f'output-{config[\"language_pair\"]}',\n","    evaluation_strategy = 'steps',\n","    eval_steps = eval_steps_interval,\n","    save_steps = eval_steps_interval,\n","    gradient_accumulation_steps = config['gradient_accumulation_steps'],\n","    learning_rate = config['learning_rate'],\n","    per_device_train_batch_size = config['train_batch_size'],\n","    per_device_eval_batch_size = config['eval_batch_size'],\n","    weight_decay = 0.01,\n","    save_total_limit = 3,\n","    num_train_epochs = config['num_train_epochs'],\n","    predict_with_generate = True,\n","    fp16 = torch.cuda.is_available(),\n","    logging_dir = f'output-{config[\"language_pair\"]}',\n","    report_to = 'wandb',\n","    run_name = f'{config[\"language_pair\"]}',\n","    load_best_model_at_end=True,\n","    metric_for_best_model = config['metric_for_best_model'],\n","    label_smoothing_factor = config['label_smoothing_factor'],\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Loading the data is done by specifying the source and destination languages for each pair, a bit redundant as redundant languages are loaded multiple times.\n","It is possible to implement this as a generator or some other type of lazy loading thing. However the dataset being small enough it is not important."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["eval_steps_interval# lang_token, path\n","## Be careful to keep the order the same for source and target dataset pairs\n","config['training_subset_paths'] = [\n","        {\n","            \"source\":{\"language\":\"ach\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-ach/train.ach\"},\n","            \"target\":{\"language\":\"en\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/train.en\"} \n","        },\n","        {\n","            \"source\":{\"language\":\"lgg\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lgg/train.lgg\"},\n","            \"target\":{\"language\":\"en\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/train.en\"} \n","        },\n","        {\n","            \"source\":{\"language\":\"lug\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/train.lug\"},\n","            \"target\":{\"language\":\"en\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/train.en\"} \n","        },\n","        {\n","            \"source\":{\"language\":\"nyn\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-nyn/train.nyn\"},\n","            \"target\":{\"language\":\"en\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/train.en\"} \n","        },\n","        \n","        {\n","            \"source\":{\"language\":\"teo\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-teo/train.teo\"},\n","            \"target\":{\"language\":\"en\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/train.en\"} \n","        }\n","    \n","   ]\n","\n","\n","    \n","config['validation_subset_paths'] = [\n","        \n","        {\n","            \"source\":{\"language\":\"ach\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-ach/val.ach\"},\n","            \"target\":{\"language\":\"en\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/val.en\"} \n","        },\n","        {\n","            \"source\":{\"language\":\"lgg\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lgg/val.lgg\"},\n","            \"target\":{\"language\":\"en\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/val.en\"} \n","        },\n","        {\n","            \"source\":{\"language\":\"lug\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/val.lug\"},\n","            \"target\":{\"language\":\"en\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/val.en\"} \n","        },\n","        {\n","            \"source\":{\"language\":\"nyn\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-nyn/val.nyn\"},\n","            \"target\":{\"language\":\"en\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/val.en\"} \n","        },\n","        {\n","            \"source\":{\"language\":\"teo\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-teo/val.teo\"},\n","            \"target\":{\"language\":\"en\",\n","                   \"path\":\"v7-dataset/v7.0/supervised/en-lug/val.en\"} \n","        }\n","    \n","   ]\n","\n","#why not luo?\n","if config['flores101_training_data']:\n","    flores_dict = {\n","        \"source\":{\n","            \"language\":\"lug\",\n","            \"path\":\"v7-dataset/v7.0/supervised/mul-en/train_flores_lug.src\"\n","        },\n","        \"target\":{\n","            \"language\":\"en\",\n","            \"path\":\"v7-dataset/v7.0/supervised/mul-en/train_flores_lug.tgt\"\n","        }\n","    }\n","    config['training_subset_paths'].append(flores_dict)\n","\n","# if config['back_translation_training_data']:\n","#     raise NotImplementedError(\"Have not split bt data by language yet\")\n","#     config['training_subset_ids'].append('back_translated')\n","\n","# Over-sample the non-religious training text\n","#config['training_subset_ids'] = config['training_subset_ids'] * 5\n","# Will oversample from interleave datasets\n","\n","if config['mt560_training_data']:\n","    mt560_list = [\n","        {\n","            \"source\":{\n","            \"language\":\"ach\",\n","            \"path\":\"v7-dataset/v7.0/supervised/mul-en/train_mt560_ach.src\"\n","            },\n","            \"target\":{\n","                \"language\":\"en\",\n","                \"path\":\"v7-dataset/v7.0/supervised/mul-en/train_mt560_ach.tgt\"\n","            }  \n","        },\n","        {\n","            \"source\":{\n","            \"language\":\"lug\",\n","            \"path\":\"v7-dataset/v7.0/supervised/mul-en/train_mt560_lug.src\"\n","            },\n","            \"target\":{\n","                \"language\":\"en\",\n","                \"path\":\"v7-dataset/v7.0/supervised/mul-en/train_mt560_lug.tgt\"\n","            }\n","        },\n","        {\n","            \"source\":{\n","            \"language\":\"nyn\",\n","            \"path\":\"v7-dataset/v7.0/supervised/mul-en/train_mt560_nyn.src\"\n","            },\n","            \"target\":{\n","                \"language\":\"en\",\n","                \"path\":\"v7-dataset/v7.0/supervised/mul-en/train_mt560_nyn.tgt\"\n","            }\n","        }        \n","\n","    ]\n","    config['training_subset_paths'].extend(mt560_list)\n","\n","# if config['named_entities_training_data']:\n","#     rasie NotImplementedError(\"NER pairs are aggregate not separate\")\n","#     config['training_subset_ids'].append('named_entities')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not os.path.exists('v7-dataset'):\n","    !wget https://sunbird-translate.s3.us-east-2.amazonaws.com/v7-dataset.zip\n","    !unzip v7-dataset.zip\n","    display.clear_output()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This is where the model is loaded. Ideally any changes/arguments here should be passed from the config dict."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = MBartForConditionalGeneration.from_pretrained(config[\"model_checkpoint\"])\n","tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n","data_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model = model) \n","metric = datasets.load_metric('sacrebleu')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Since we're using a pretrained model with its own tokenizer/bpe encoding we need to either create new tokens and expand the existing model embeddings,\n","as specified in this link https://www.depends-on-the-definition.com/how-to-add-new-tokens-to-huggingface-transformers/ (Feel free to add this if you need it). Or since the mBART50 is trained on 50 languages, 49 of which we don't need. Then we could simply proceed to use the tokens of unsused languages for our own languages.\n","\n","From a code cleanliness prespective it is better to change the tokenizer's values for the different languages so that when we use tokenizer.encode(\"teo\") we get a numerical value directly, but replacing all incidences of \"teo\" with \"ar_AR\" or similar would also work."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if config[\"recycle_language_tokens\"]:\n","    token_conversion_dict = {\n","        \"teo\": 'ar_AR' ,\n","        \"ach\": 'cs_CZ',\n","        \"lug\": 'de_DE',\n","        \"lgg\": 'es_XX',\n","        \"nyn\": 'et_EE',\n","        \"en\": 'en_XX'\n","        \n","     }\n","else:\n","    raise NotImplementedError(\"Code to add tokens and resize embedding layer not added\")\n","    # If you want to add it refer to https://www.depends-on-the-definition.com/how-to-add-new-tokens-to-huggingface-transformers/"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Since validation takes long to run, we limit validation data based on the validation_cutoff argument, that we provide a value for in config[\"validation_samples_per_language\"]. such that for each file in the config[\"validation_subset_paths\"] we only load a subset of the validation data, whether to discard the rest or add to the training set is controlled by the config[\"validation_train_merge\"] value (True for use extra validation data in training, False for discard)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def _file_to_list(path):\n","    with open(path) as file:\n","        lines = file.readlines()\n","        lines = [line.rstrip() for line in lines]\n","        return lines\n","\n","def dataset_from_folders(pair_dicts_list, language_token_dict, validation_cutoff = 0,mode = \"cutoff_maximum\"):\n","\n","    list_of_pairs = []\n","    for language_pair_dict in pair_dicts_list:\n","        src_language = language_pair_dict[\"source\"][\"language\"]\n","        src_scentences = _file_to_list(language_pair_dict[\"source\"][\"path\"])\n","        tgt_language = language_pair_dict[\"target\"][\"language\"]\n","        tgt_scentences = _file_to_list(language_pair_dict[\"target\"][\"path\"])\n","                \n","        if validation_cutoff:\n","            if mode == \"cutoff_maximum\":\n","                src_scentences = src_scentences[:validation_cutoff]\n","                tgt_scentences = tgt_scentences[:validation_cutoff]\n","            elif mode == \"cutoff_minimum\":\n","                src_scentences = src_scentences[validation_cutoff:]\n","                tgt_scentences = tgt_scentences[validation_cutoff:]\n","\n","        # pairs = {'translation': [{src_language: s,\n","        #                     tgt_language: t}\n","        #                      for s, t in zip(src_scentences, tgt_scentences)]}\n","        \n","        src_scentences = [language_token_dict[src_language] + \" \" + src for src in src_scentences]\n","        tgt_scentences = [language_token_dict[tgt_language] + \" \" + tgt for tgt in tgt_scentences]\n","        \n","\n","        pairs = {'translation': [{\"src\": s,\n","                            \"tgt\": t}\n","                             for s, t in zip(src_scentences, tgt_scentences)]}\n","\n","        list_of_pairs.append(datasets.Dataset.from_dict(pairs))\n","\n","    return list_of_pairs\n","\n","def dataset_from_src_tgt_files(data_dir, dataset_id, validation_cutoff = 0, mode = \"train\"):\n","    \"\"\"\n","        validation_cutoff: use first n lines as validation\n","    \"\"\"\n","\n","    path = os.path.join(data_dir, dataset_id)\n","    source, target = [_file_to_list(path + '.src'),\n","                      _file_to_list(path + '.tgt')]\n","    if mode == \"cutoff_maximum\":\n","        source = source[:validation_cutoff]\n","        target = target[:validation_cutoff]\n","    elif mode == \"cutoff_minimum\":\n","        source = source[validation_cutoff:]\n","        target = target[validation_cutoff:]\n","\n","    #pairs = {'translation': [{config['source_language']: s,\n","    #                          config['target_language']: t}\n","    #                         for s, t in zip(source, target)]}\n","    \n","    pairs = {'translation': [{config['source_language']: s,\n","                             config['target_language']: t}\n","                            for s, t in zip(source, target)]}\n","    \n","    return datasets.Dataset.from_dict(pairs)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The MT560 dataset is larger than the others with a focus on religious data. The luganda-mt560 is especially massive, we undersample the mt560 scentence pairs using the sample_probabilities array, which tells the datasets.interleave_datasets how much to sample from each dataset to make the final combined dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_subsets = dataset_from_folders(config['training_subset_paths'],\n","                                        token_conversion_dict,\n","                                        validation_cutoff = 0)\n","if config[\"validation_train_merge\"]: \n","    extra_training_data = dataset_from_folders(config['validation_subset_paths'], \n","                                               token_conversion_dict,\n","                                               validation_cutoff = config['validation_samples_per_language'],\n","                                               mode = \"cutoff_minimum\")\n","    training_subsets.extend(extra_training_data)\n","\n","training_subsets = [s.shuffle() for s in training_subsets]\n","\n","\n","sample_probabilities = np.array([len(s) for s in training_subsets])\n","sample_probabilities[6] = sample_probabilities[6]//10 #downsample mt560 ach by a factor of 10\n","sample_probabilities[7] = sample_probabilities[7]//20 #downsample mt560 lug by a factor of 20\n","sample_probabilities[8] = sample_probabilities[8]//10 #downsample mt560 nyn by a factor of 10\n","\n","sample_probabilities = sample_probabilities / np.sum(sample_probabilities)\n","\n","train_data_raw = datasets.interleave_datasets( \n","    training_subsets, sample_probabilities)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_subsets = dataset_from_folders(config['validation_subset_paths'], \n","                                                token_conversion_dict,\n","                                               validation_cutoff = config['validation_samples_per_language'],\n","                                               mode = \"cutoff_maximum\")\n","    \n","\n","validation_data_raw = datasets.concatenate_datasets(validation_subsets)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sentence_format(input):\n","    '''Ensure capital letter at the start and full stop at the end.'''\n","    input = input[0].capitalize() + input[1:]\n","    if input[-1] not in ['.', '!', '?']:\n","        input = input + '.'\n","    return input\n","\n","def preprocess(examples):\n","    normalizer = sacremoses.MosesPunctNormalizer()\n","    inputs = [ex[\"src\"] for ex in examples['translation']]\n","    targets = [ex[\"tgt\"] for ex in examples['translation']]\n","\n","    inputs = [sentence_format(normalizer.normalize(text))\n","              for text in inputs]\n","    targets = [sentence_format(normalizer.normalize(text))\n","               for text in targets]\n","    \n","    model_inputs = tokenizer(\n","        inputs, max_length=config['max_input_length'], truncation=True)\n","\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(\n","            targets, max_length=config['max_target_length'], truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","    return model_inputs\n","\n","def postprocess(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","    return preds, labels\n","\n","def compute_metrics(eval_preds, eval_languages, samples_per_language):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","        \n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess(decoded_preds, decoded_labels)\n","    \n","    result = {}\n","    for i, lang in enumerate(eval_languages):\n","        result_subset = metric.compute(\n","            predictions=decoded_preds[i*samples_per_language:(i+1)*samples_per_language],\n","            references=decoded_labels[i*samples_per_language:(i+1)*samples_per_language])\n","        result[f\"BLEU_{lang}\"] = result_subset[\"score\"]\n","        \n","    result[\"BLEU_mean\"] = np.mean([result[f\"BLEU_{lang}\"] for lang in eval_languages])\n","    \n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This is where the tokenizer sets the target language token, for the case of generating multiple languages from this same model, we need a way to be able to inject a different language token for each desired output language."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer.tgt_lang = 'en_XX'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","train_data  = train_data_raw.map(\n","    preprocess, remove_columns=[\"translation\"], batched=True)\n","\n","validation_data  = validation_data_raw.map(\n","    preprocess, remove_columns=[\"translation\"], batched=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.config.use_cache = False"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Cross your fingers, say your prayers, train !"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.environ[\"WANDB_API_KEY\"] = \"ENTER YOUR WANDB API KEY HERE\"\n","wandb.init(project=config['wandb_project'],entity=config[\"wandb_entity\"], config=config)\n","\n","trainer = transformers.Seq2SeqTrainer(\n","    model,\n","    config['train_settings'],\n","    train_dataset = train_data,\n","    eval_dataset = validation_data,\n","    data_collator = data_collator,\n","    tokenizer = tokenizer,\n","    compute_metrics = lambda x: compute_metrics(\n","        x, config['eval_languages'], config['validation_samples_per_language']),\n","    callbacks = [transformers.EarlyStoppingCallback(early_stopping_patience = 5)],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if config['eval_pretrained_model']:\n","    trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.save_model(\"/kaggle/working/best_mBART_salt\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
