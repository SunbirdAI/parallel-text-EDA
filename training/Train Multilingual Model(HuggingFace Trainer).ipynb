{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython import display\n!pip install transformers\n!pip install sacrebleu\n!pip install sacremoses\n!pip install datasets\n!pip install wandb\n!pip install sentencepiece\ndisplay.clear_output()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-08T13:32:22.950391Z","iopub.execute_input":"2023-02-08T13:32:22.950750Z","iopub.status.idle":"2023-02-08T13:33:23.536574Z","shell.execute_reply.started":"2023-02-08T13:32:22.950719Z","shell.execute_reply":"2023-02-08T13:33:23.535221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\nfrom IPython import display\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport sentencepiece\nimport sacrebleu\nimport sacremoses\nfrom tqdm import tqdm\nimport transformers\nimport torch\nimport wandb\nimport glob","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:33:23.539147Z","iopub.execute_input":"2023-02-08T13:33:23.539556Z","iopub.status.idle":"2023-02-08T13:33:26.872976Z","shell.execute_reply.started":"2023-02-08T13:33:23.539517Z","shell.execute_reply":"2023-02-08T13:33:26.871793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:33:26.874975Z","iopub.execute_input":"2023-02-08T13:33:26.875351Z","iopub.status.idle":"2023-02-08T13:33:26.943579Z","shell.execute_reply.started":"2023-02-08T13:33:26.875313Z","shell.execute_reply":"2023-02-08T13:33:26.942448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameters for mul-en models\nconfig = {\n    'source_language': 'mul',\n    'target_language': 'en',\n    'metric_for_best_model': 'loss',\n    'train_batch_size': 20,\n    'gradient_accumulation_steps': 150,\n    'max_input_length': 128,\n    'max_target_length': 128,\n    'validation_samples_per_language': 500,\n    'eval_batch_size': 16,\n    'eval_languages': [\"ach\", \"lgg\", \"lug\", \"nyn\", \"teo\"],\n    'eval_pretrained_model': True,\n    'learning_rate': 0.0001,\n    'num_train_epochs': 10,\n    'label_smoothing_factor': 0.1,\n    'flores101_training_data': True,\n    'mt560_training_data': True,\n    'back_translation_training_data': True,\n    'back_translation_model_checkpoint': '/content/gdrive/Shareddrives/Sunbird AI/Projects/African Language Technology/Models/en-mul-ethereal-valley',\n    'back_translation_token': True,\n    'forward_translation': True,\n    'forward_translation_percentage':0.1,\n    #'forward_translation_token': True,\n    'out_of_domain_token': True,\n    'named_entities_training_data': False,\n}\n\nconfig['language_pair'] = f'{config[\"source_language\"]}-{config[\"target_language\"]}'\nconfig['wandb_project'] = f'salt-fwd-bck-ood'\nconfig['wandb_entity'] = f'sunbird'\n\n#config['model_checkpoint'] = f'Helsinki-NLP/opus-mt-{config[\"language_pair\"]}'\nconfig['model_checkpoint'] = '/kaggle/input/nmt-backtranslation-ood-forward-translation/output-mul-en/checkpoint-800'\n\n# What training data to use\nconfig['data_dir'] = f'v7-dataset/v7.0/supervised/{config[\"language_pair\"]}/'\n\n# Evaluate roughly every 10 minutes\neval_steps_interval = 350 * 60 * 7 / (config['gradient_accumulation_steps']\n                                      * config['train_batch_size'])\n\neval_steps_interval = 10 * max(1, int(eval_steps_interval / 10))\n\nprint(f'Evaluating every {eval_steps_interval} training steps.')\n\nconfig['train_settings'] = transformers.Seq2SeqTrainingArguments(\n    f'output-{config[\"language_pair\"]}',\n    evaluation_strategy = 'steps',\n    eval_steps = eval_steps_interval,\n    save_steps = eval_steps_interval,\n    gradient_accumulation_steps = config['gradient_accumulation_steps'],\n    learning_rate = config['learning_rate'],\n    per_device_train_batch_size = config['train_batch_size'],\n    per_device_eval_batch_size = config['eval_batch_size'],\n    weight_decay = 0.01,\n    save_total_limit = 3,\n    num_train_epochs = config['num_train_epochs'],\n    predict_with_generate = True,\n    fp16 = torch.cuda.is_available(),\n    logging_dir = f'output-{config[\"language_pair\"]}',\n    report_to = 'wandb',\n    run_name = f'{config[\"source_language\"]}-{config[\"target_language\"]}',\n    load_best_model_at_end=True,\n    metric_for_best_model = config['metric_for_best_model'],\n    label_smoothing_factor = config['label_smoothing_factor']\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:33:26.947981Z","iopub.execute_input":"2023-02-08T13:33:26.948657Z","iopub.status.idle":"2023-02-08T13:33:31.448538Z","shell.execute_reply.started":"2023-02-08T13:33:26.948629Z","shell.execute_reply":"2023-02-08T13:33:31.447532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config['training_subset_ids'] = [\n    'train', 'train_ai4d',\n]\n\nif config['forward_translation']:\n    config['training_subset_ids'].extend(\n    [\"forward_\" + training_subset for training_subset in config['training_subset_ids']]\n    )\n\n\nconfig['validation_subset_ids'] = [\n    'val_ach', 'val_lgg', 'val_lug', 'val_nyn', 'val_teo'\n]\n\n\nif config['flores101_training_data']:\n    config['training_subset_ids'] .append('train_flores_lug')\n\nif config['back_translation_training_data']:\n    config['training_subset_ids'].append('back_translated')\n\n# Over-sample the non-religious training text\nconfig['training_subset_ids'] = config['training_subset_ids'] * 5\n\nif config['mt560_training_data']:\n    config['training_subset_ids'].extend([\n        'train_mt560_lug', 'train_mt560_ach', 'train_mt560_nyn',\n    ])\n    \n\n\nif config['named_entities_training_data']:\n    config['training_subset_ids'].append('named_entities')","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:33:31.449988Z","iopub.execute_input":"2023-02-08T13:33:31.450391Z","iopub.status.idle":"2023-02-08T13:33:31.460294Z","shell.execute_reply.started":"2023-02-08T13:33:31.450353Z","shell.execute_reply":"2023-02-08T13:33:31.459242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/\")\nif not os.path.exists('v7-dataset'):\n    !wget https://sunbird-translate.s3.us-east-2.amazonaws.com/v7-dataset.zip\n    !unzip v7-dataset.zip\n    display.clear_output()\n!cp /kaggle/input/salt-extra-mul-en/* /kaggle/working/v7-dataset/v7.0/supervised/mul-en/","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:33:31.461850Z","iopub.execute_input":"2023-02-08T13:33:31.462242Z","iopub.status.idle":"2023-02-08T13:33:45.763816Z","shell.execute_reply.started":"2023-02-08T13:33:31.462169Z","shell.execute_reply":"2023-02-08T13:33:45.762085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _file_to_list(path):\n    with open(path) as file:\n        lines = file.readlines()\n        lines = [line.rstrip() for line in lines]\n        return lines\n    \ndef dataset_from_src_tgt_files(data_dir, dataset_id, read_first_n = 0):\n    path = os.path.join(data_dir, dataset_id)\n    \n    if config[\"forward_translation\"] and dataset_id.startswith(\"forward_\"):\n        source, target = [_file_to_list(path.replace(\"forward_\", \"\") + '.src'),\n                      _file_to_list(path.replace(\"forward_\", \"\") + '.src')]\n        source = [\">>fwd<< \" + scentence for scentence in source ]\n        \n        if read_first_n:\n            source = source[:read_first_n]\n            target = target[:read_first_n]\n        \n        if config[\"forward_translation_percentage\"] < 1.0:\n            cutoff_idx = int(len(source)*config[\"forward_translation_percentage\"])\n            c = list(zip(source, target))\n            random.shuffle(c)\n            source, target = zip(*c)\n            source = source[:cutoff_idx]\n            target = target[:cutoff_idx]\n\n        \n    else:\n        source, target = [_file_to_list(path + '.src'),\n                      _file_to_list(path + '.tgt')]\n        \n        if read_first_n:\n            source = source[:read_first_n]\n            target = target[:read_first_n]\n        \n        if config['out_of_domain_token'] and \"mt560\" in dataset_id :\n            source = [\">>ood<< \" + scentence for scentence in source]\n        \n    \n        \n    if dataset_id == \"back_translated\" and config[\"back_translation_token\"]:\n        source = [\">>bck<< \" + scentence for scentence in source ] #Do we add a space after token?\n        \n    \n    pairs = {'translation': [{config['source_language']: s,\n                              config['target_language']: t}\n                             for s, t in zip(source, target)]}\n    \n    return datasets.Dataset.from_dict(pairs)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:33:45.766232Z","iopub.execute_input":"2023-02-08T13:33:45.767035Z","iopub.status.idle":"2023-02-08T13:33:45.792362Z","shell.execute_reply.started":"2023-02-08T13:33:45.766990Z","shell.execute_reply":"2023-02-08T13:33:45.788538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_subsets = [dataset_from_src_tgt_files(config['data_dir'], id)\n                    for id in config['training_subset_ids']]\ntraining_subsets = [s.shuffle() for s in training_subsets]\n\nsample_probabilities = np.array([len(s) for s in training_subsets])\nsample_probabilities = sample_probabilities / np.sum(sample_probabilities)\n\ntrain_data_raw = datasets.interleave_datasets(\n    training_subsets, sample_probabilities)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:33:45.794086Z","iopub.execute_input":"2023-02-08T13:33:45.795303Z","iopub.status.idle":"2023-02-08T13:34:45.528686Z","shell.execute_reply.started":"2023-02-08T13:33:45.795259Z","shell.execute_reply":"2023-02-08T13:34:45.527670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_subsets = [dataset_from_src_tgt_files(\n                                            config['data_dir'], \n                                            id, \n                                            read_first_n = config['validation_samples_per_language']\n                                            )\n                                        for id in config['validation_subset_ids']]\n\n\nvalidation_data_raw = datasets.concatenate_datasets(validation_subsets)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:34:45.530147Z","iopub.execute_input":"2023-02-08T13:34:45.530740Z","iopub.status.idle":"2023-02-08T13:34:45.563978Z","shell.execute_reply.started":"2023-02-08T13:34:45.530702Z","shell.execute_reply":"2023-02-08T13:34:45.563082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentence_format(input):\n    '''Ensure capital letter at the start and full stop at the end.'''\n    input = input[0].capitalize() + input[1:]\n    if input[-1] not in ['.', '!', '?']:\n        input = input + '.'\n    return input\n\ndef preprocess(examples):\n    normalizer = sacremoses.MosesPunctNormalizer()\n    \n    inputs = [ex[config['source_language']] for ex in examples['translation']]\n    targets = [ex[config['target_language']] for ex in examples['translation']]\n\n    inputs = [sentence_format(normalizer.normalize(text))\n              for text in inputs]\n    targets = [sentence_format(normalizer.normalize(text))\n               for text in targets]\n    \n    model_inputs = tokenizer(\n        inputs, max_length=config['max_input_length'], truncation=True)\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets, max_length=config['max_target_length'], truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n    return model_inputs\n\ndef postprocess(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds, eval_languages, samples_per_language):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n        \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess(decoded_preds, decoded_labels)\n    \n    result = {}\n    for i, lang in enumerate(eval_languages):\n        result_subset = metric.compute(\n            predictions=decoded_preds[i*samples_per_language:(i+1)*samples_per_language],\n            references=decoded_labels[i*samples_per_language:(i+1)*samples_per_language])\n        result[f\"BLEU_{lang}\"] = result_subset[\"score\"]\n        \n    result[\"BLEU_mean\"] = np.mean([result[f\"BLEU_{lang}\"] for lang in eval_languages])\n    \n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:34:45.567722Z","iopub.execute_input":"2023-02-08T13:34:45.568000Z","iopub.status.idle":"2023-02-08T13:34:45.582089Z","shell.execute_reply.started":"2023-02-08T13:34:45.567974Z","shell.execute_reply":"2023-02-08T13:34:45.581124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = transformers.AutoModelForSeq2SeqLM.from_pretrained(config['model_checkpoint'])\ntokenizer = transformers.AutoTokenizer.from_pretrained(config['model_checkpoint'])\ndata_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model = model) \nmetric = datasets.load_metric('sacrebleu')","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:34:45.583725Z","iopub.execute_input":"2023-02-08T13:34:45.584115Z","iopub.status.idle":"2023-02-08T13:35:39.136029Z","shell.execute_reply.started":"2023-02-08T13:34:45.584053Z","shell.execute_reply":"2023-02-08T13:35:39.135163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config['back_translation_token']: \n    #replacing र\n    tokenizer.encoder[\">>bck<<\"] = tokenizer.encoder[\"र\"] #or plug 735\n    \nif config['forward_translation']: \n    tokenizer.encoder[\">>fwd<<\"] = tokenizer.encoder[\"را\"] #or plug 1293\n    \n    \nif config['out_of_domain_token']: \n    tokenizer.encoder[\">>ood<<\"] = tokenizer.encoder[\"ش\"] #or plug 471\n    \n#display.clear_output()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:35:39.137565Z","iopub.execute_input":"2023-02-08T13:35:39.138151Z","iopub.status.idle":"2023-02-08T13:35:39.146104Z","shell.execute_reply.started":"2023-02-08T13:35:39.138114Z","shell.execute_reply":"2023-02-08T13:35:39.145150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config['target_language'] == 'mul':\n    replacements = {'bck': 'kin',\n                    'lgg': 'lin',\n                    'ach': 'tso',\n                    'teo': 'som',\n                    'luo': 'sna',\n\n                   }\n    for r in replacements:\n        if (f'>>{r}<<' not in tokenizer.encoder and\n            f'>>{replacements[r]}<<' in tokenizer.encoder):\n            tokenizer.encoder[f\">>{r}<<\"] = tokenizer.encoder[f\">>{replacements[r]}<<\"]\n            del tokenizer.encoder[f\">>{replacements[r]}<<\"]\n\n    # Check that all the evaluation language codes are mapped to something.\n    for r in config['eval_languages']:\n        if f'>>{r}<<' not in tokenizer.encoder:\n            raise ValueError(f'Language code {r} not found in the encoder.')","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:35:39.147823Z","iopub.execute_input":"2023-02-08T13:35:39.148240Z","iopub.status.idle":"2023-02-08T13:35:39.157691Z","shell.execute_reply.started":"2023-02-08T13:35:39.148206Z","shell.execute_reply":"2023-02-08T13:35:39.156757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data  = train_data_raw.map(\n    preprocess, remove_columns=[\"translation\"], batched=True)\n\nvalidation_data  = validation_data_raw.map(\n    preprocess, remove_columns=[\"translation\"], batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:35:39.159207Z","iopub.execute_input":"2023-02-08T13:35:39.159828Z","iopub.status.idle":"2023-02-08T13:44:59.646919Z","shell.execute_reply.started":"2023-02-08T13:35:39.159792Z","shell.execute_reply":"2023-02-08T13:44:59.645849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#optimizer = transformers.AdamW( model.parameters(), lr = config[\"learning_rate\"])\n#total_steps_warmup = (config['num_train_epochs'] * len(train_data))//2\n#scheduler = transformers.get_linear_schedule_with_warmup( optimizer, num_warmup_steps=total_steps_warmup, num_training_steps=total_steps_warmup ) \n#optimizers = (optimizer, scheduler)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:50:31.669956Z","iopub.execute_input":"2023-02-08T13:50:31.670591Z","iopub.status.idle":"2023-02-08T13:50:31.697855Z","shell.execute_reply.started":"2023-02-08T13:50:31.670544Z","shell.execute_reply":"2023-02-08T13:50:31.696901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_key\")\nimport os\nos.environ[\"WANDB_API_KEY\"] = secret_value_0\nwandb.init(project=config['wandb_project'],entity=config[\"wandb_entity\"], config=config)\n\ntrainer = transformers.Seq2SeqTrainer(\n    model,\n    config['train_settings'],\n    train_dataset = train_data,\n    eval_dataset = validation_data,\n    data_collator = data_collator,\n    tokenizer = tokenizer,\n    #optimizers= optimizers,\n    compute_metrics = lambda x: compute_metrics(\n        x, config['eval_languages'], config['validation_samples_per_language']),\n    callbacks = [transformers.EarlyStoppingCallback(early_stopping_patience = 5)],\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:50:33.802104Z","iopub.execute_input":"2023-02-08T13:50:33.802780Z","iopub.status.idle":"2023-02-08T13:51:17.493807Z","shell.execute_reply.started":"2023-02-08T13:50:33.802730Z","shell.execute_reply":"2023-02-08T13:51:17.492682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:51:28.830974Z","iopub.execute_input":"2023-02-08T13:51:28.831733Z","iopub.status.idle":"2023-02-08T13:52:19.009832Z","shell.execute_reply.started":"2023-02-08T13:51:28.831691Z","shell.execute_reply":"2023-02-08T13:52:19.008414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config['eval_pretrained_model']:\n    trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:44:59.957763Z","iopub.status.idle":"2023-02-08T13:44:59.958595Z","shell.execute_reply.started":"2023-02-08T13:44:59.958329Z","shell.execute_reply":"2023-02-08T13:44:59.958356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config['eval_pretrained_model']:\n    trainer.evaluate(num_beams=4)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T13:44:59.960255Z","iopub.status.idle":"2023-02-08T13:44:59.961010Z","shell.execute_reply.started":"2023-02-08T13:44:59.960755Z","shell.execute_reply":"2023-02-08T13:44:59.960780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{},"execution_count":null,"outputs":[]}]}