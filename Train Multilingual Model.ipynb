{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b06e57",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ce02158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "!pip install transformers\n",
    "!pip install sacrebleu\n",
    "!pip install sacremoses\n",
    "!pip install datasets\n",
    "!pip install wandb\n",
    "!pip install sentencepiece\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36461e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sentencepiece\n",
    "import sacrebleu\n",
    "import sacremoses\n",
    "import tqdm\n",
    "import transformers\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c875def4-aa4a-46b1-ac1b-71e8702dcdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea76c66",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Alternatives for pre-training when translating to English: `Helsinki-NLP/opus-mt-lg-en`, `Helsinki-NLP/opus-mt-mul-en`.\n",
    "\n",
    "Note 1: when training on V100 GPUs, there is more memory and `train_batch_size` can be increased (to 64?). If this is done then `gradient_accumulation_steps` should then be decreased accordingly, so that there is the same effective batch size.\n",
    "\n",
    "Note 2: there is little difference in BLEU score when using a test set of 500 vs 1000 sentences per language. For rapid parameter tuning, we can therefore use `config['validation_samples_per_language'] = 500`, and then set it to 1000 for the best model config to report numbers in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b27659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating every 100 training steps.\n"
     ]
    }
   ],
   "source": [
    "# Parameters for mul-en models\n",
    "config = {\n",
    "    'source_language': 'en',\n",
    "    'target_language': 'mul',\n",
    "    'metric': 'sacrebleu',\n",
    "    'model_checkpoint': 'Helsinki-NLP/opus-mt-en-mul',\n",
    "    'train_batch_size': 16,\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'eval_batch_size': 16,\n",
    "    'max_input_length': 128,\n",
    "    'max_target_length': 128,\n",
    "    'input_prefix': '',\n",
    "    'validation_samples_per_language': 500,\n",
    "    'eval_languages': [\"ach\", \"lgg\", \"lug\", \"nyn\", \"teo\"],\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 20,\n",
    "    'label_smoothing_factor': 0.1,\n",
    "}\n",
    "\n",
    "# Adjustments for en-mul models\n",
    "if config['target_language'] == 'mul':\n",
    "    config['learning_rate'] = 5e-4\n",
    "    config['gradient_accumulation_steps'] = 64\n",
    "\n",
    "\n",
    "config['language_pair'] = f'{config[\"source_language\"]}-{config[\"target_language\"]}'\n",
    "config['wandb_project'] = f'sunbird-translate-{config[\"language_pair\"]}'\n",
    "config['model_checkpoint'] = f'Helsinki-NLP/opus-mt-{config[\"language_pair\"]}'\n",
    "\n",
    "# Evaluate roughly every 10 minutes\n",
    "eval_steps_interval = 220 * 60 * 7 / (config['gradient_accumulation_steps']\n",
    "                                      * config['train_batch_size'])\n",
    "\n",
    "eval_steps_interval = 100 * max(1, int(eval_steps_interval / 100))\n",
    "\n",
    "print(f'Evaluating every {eval_steps_interval} training steps.')\n",
    "\n",
    "config['train_settings'] = transformers.Seq2SeqTrainingArguments(\n",
    "    f'output-{config[\"language_pair\"]}',\n",
    "    evaluation_strategy = 'steps',\n",
    "    eval_steps = eval_steps_interval,\n",
    "    save_steps = eval_steps_interval,\n",
    "    gradient_accumulation_steps = config['gradient_accumulation_steps'],\n",
    "    learning_rate = config['learning_rate'],\n",
    "    per_device_train_batch_size = config['train_batch_size'],\n",
    "    per_device_eval_batch_size = config['eval_batch_size'],\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 3,\n",
    "    num_train_epochs = config['num_train_epochs'],\n",
    "    predict_with_generate = True,\n",
    "    fp16 = torch.cuda.is_available(),\n",
    "    logging_dir = f'output-{config[\"language_pair\"]}',\n",
    "    report_to = 'wandb',\n",
    "    run_name = f'{config[\"source_language\"]}-{config[\"target_language\"]}',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model = 'loss',\n",
    "    label_smoothing_factor = config['label_smoothing_factor']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f607807-40be-4cda-8fa7-67abb72f0234",
   "metadata": {},
   "source": [
    "# Set up datasets\n",
    "\n",
    "Download the raw text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29dcda22-59ab-49b3-b2fd-5a8b200dead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('v7-dataset'):\n",
    "    !wget https://sunbird-translate.s3.us-east-2.amazonaws.com/v7-dataset.zip\n",
    "    !unzip v7-dataset.zip\n",
    "    display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332961c-fb4b-4249-bac1-2c8e4384faa2",
   "metadata": {},
   "source": [
    "Create a training set by interleaving separate training subsets.\n",
    "\n",
    "Notes:\n",
    "* This includes MT560 which has many examples (484,925), but which is biased towards religious text so we sample from it sparsely.\n",
    "* We just use a 2-way train/test split for this experiment, so include the validation sentences in with the training set.\n",
    "* LGG, ACH and TEO are oversampled a little by duplicating the validation sets, as a simple way to correct for there being more LUG and NYN training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1db95e17-5b8a-473f-b38f-2303ca7d9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = f'v7-dataset/v7.0/supervised/{config[\"language_pair\"]}/'\n",
    "TRAINING_SUBSET_IDS = ['train', 'train_mt560', 'train_ai4d', 'train_flores',\n",
    "                       'val_ach', 'val_lgg', 'val_lug', 'val_nyn', 'val_teo',\n",
    "                       'val_ach', 'val_teo', 'val_lgg']\n",
    "TRAINING_SUBSETS_SAMPLE_RATIO = [83770, 80000, 15021, 4018,\n",
    "                                 4126, 4126, 4126, 4126, 4126,\n",
    "                                 4126, 4126, 4126]\n",
    "\n",
    "def _file_to_list(path):\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "        return lines\n",
    "    \n",
    "def dataset_from_src_tgt_files(data_dir, dataset_id, read_first_n = 0):\n",
    "    path = os.path.join(data_dir, dataset_id)\n",
    "    source, target = [_file_to_list(path + '.src'), _file_to_list(path + '.tgt')]\n",
    "    if read_first_n:\n",
    "        source = source[:read_first_n]\n",
    "        target = target[:read_first_n]\n",
    "    pairs = {'translation': [{config['source_language']: s, config['target_language']: t}\n",
    "                             for s, t in zip(source, target)]}\n",
    "    return datasets.Dataset.from_dict(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50887e34-b0c7-40d2-8e00-2cfa525a3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_subsets = [dataset_from_src_tgt_files(DATA_DIR, id) for id in TRAINING_SUBSET_IDS]\n",
    "training_subsets = [s.shuffle() for s in training_subsets]\n",
    "sample_probabilities = np.array(TRAINING_SUBSETS_SAMPLE_RATIO) / np.sum(TRAINING_SUBSETS_SAMPLE_RATIO)\n",
    "train_data_raw = datasets.interleave_datasets(training_subsets, sample_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd838152-63c9-45ef-830d-564d6d34b72f",
   "metadata": {},
   "source": [
    "Make the separate validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e273c90-378b-46fd-b07b-52a3caa3725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_subsets = [dataset_from_src_tgt_files(\n",
    "    DATA_DIR, f'test_{lang}', read_first_n = config['validation_samples_per_language'])\n",
    "    for lang in config['eval_languages']]\n",
    "validation_data_raw = datasets.concatenate_datasets(validation_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b26f9",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Note that whatever pre-processing we do here (punctuation normalisation and ensuring sentence case), we should also do at test-time when running the model on real queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dda8c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_format(input):\n",
    "    '''Ensure capital letter at the start and full stop at the end.'''\n",
    "    input = input[0].capitalize() + input[1:]\n",
    "    if input[-1] not in ['.', '!', '?']:\n",
    "        input = input + '.'\n",
    "    return input\n",
    "\n",
    "def preprocess(examples):\n",
    "    normalizer = sacremoses.MosesPunctNormalizer()\n",
    "    \n",
    "    inputs = [ex[config['source_language']] for ex in examples['translation']]\n",
    "    targets = [ex[config['target_language']] for ex in examples['translation']]\n",
    "\n",
    "    inputs = [sentence_format(normalizer.normalize(text))\n",
    "              for text in inputs]\n",
    "    targets = [sentence_format(normalizer.normalize(text))\n",
    "               for text in targets]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=config['max_input_length'], truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, max_length=config['max_target_length'], truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "def postprocess(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds, eval_languages, samples_per_language):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess(decoded_preds, decoded_labels)\n",
    "    \n",
    "    result = {}\n",
    "    for i, lang in enumerate(eval_languages):\n",
    "        result_subset = metric.compute(\n",
    "            predictions=decoded_preds[i*samples_per_language:(i+1)*samples_per_language],\n",
    "            references=decoded_labels[i*samples_per_language:(i+1)*samples_per_language])\n",
    "        result[f\"BLEU_{lang}\"] = result_subset[\"score\"]\n",
    "        \n",
    "    result[\"BLEU_mean\"] = np.mean([result[f\"BLEU_{lang}\"] for lang in eval_languages])\n",
    "    \n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a392ae",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Instantiate the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59977cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(config['model_checkpoint'])\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config['model_checkpoint'])\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model = model) \n",
    "metric = datasets.load_metric(config['metric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7e240-2824-4862-80e0-5dbb4c0de094",
   "metadata": {},
   "source": [
    "For multiple language outputs, we need to make sure the language codes have some mapping in the encoder. We can re-use the token indices of some other language codes in the pre-trained model that we don't need.\n",
    "\n",
    "In `Helsinki-NLP/opus-mt-en-mul`, only Luganda (`lug`) is already supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a6c9a37-c7a7-4954-a6d7-fe3d44dfb1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['target_language'] == 'mul':\n",
    "    replacement_codes = {'nyn': 'kin',\n",
    "                         'lgg': 'lin',\n",
    "                         'ach': 'tso',\n",
    "                         'teo': 'som',\n",
    "                         'luo': 'sna'}\n",
    "    for r in replacement_codes:\n",
    "        if (f'>>{r}<<' not in tokenizer.encoder and\n",
    "            f'>>{replacement_codes[r]}<<' in tokenizer.encoder):\n",
    "            tokenizer.encoder[f\">>{r}<<\"] = tokenizer.encoder[f\">>{replacement_codes[r]}<<\"]\n",
    "            del tokenizer.encoder[f\">>{replacement_codes[r]}<<\"]\n",
    "\n",
    "    # Check that all the evaluation language codes are mapped to something.\n",
    "    for r in config['eval_languages']:\n",
    "        if f'>>{r}<<' not in tokenizer.encoder:\n",
    "            raise ValueError(f'Language code {r} not found in the encoder.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9331d9b",
   "metadata": {},
   "source": [
    "Pre-process the raw text datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffd05d31-b8e2-4049-bbb4-8abbeafacbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4f77c89bd0403abfa4a43a14a8b3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2352d6ae97443cb822988a74a031c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data  = train_data_raw.map(\n",
    "    preprocess, remove_columns=[\"translation\"], batched=True)\n",
    "\n",
    "validation_data  = validation_data_raw.map(\n",
    "    preprocess, remove_columns=[\"translation\"], batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ec59a",
   "metadata": {},
   "source": [
    "Launch the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb307074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjqug\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/jqug/sunbird-translate-en-mul/runs/1qahsffm\" target=\"_blank\">ethereal-valley-7</a></strong> to <a href=\"https://wandb.ai/jqug/sunbird-translate-en-mul\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=config['wandb_project'], config=config)\n",
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model,\n",
    "    config['train_settings'],\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = validation_data,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = lambda x: compute_metrics(\n",
    "        x, config['eval_languages'], config['validation_samples_per_language']),\n",
    "    callbacks = [transformers.EarlyStoppingCallback(early_stopping_patience = 3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2373b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 211923\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1024\n",
      "  Gradient Accumulation steps = 64\n",
      "  Total optimization steps = 4120\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1700' max='4120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1700/4120 3:08:24 < 4:28:30, 0.15 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu Ach</th>\n",
       "      <th>Bleu Lgg</th>\n",
       "      <th>Bleu Lug</th>\n",
       "      <th>Bleu Nyn</th>\n",
       "      <th>Bleu Teo</th>\n",
       "      <th>Bleu Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.309345</td>\n",
       "      <td>8.593900</td>\n",
       "      <td>5.123700</td>\n",
       "      <td>17.236500</td>\n",
       "      <td>8.025800</td>\n",
       "      <td>6.397900</td>\n",
       "      <td>9.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.019797</td>\n",
       "      <td>11.018900</td>\n",
       "      <td>10.405000</td>\n",
       "      <td>18.448500</td>\n",
       "      <td>10.360200</td>\n",
       "      <td>11.453300</td>\n",
       "      <td>12.337200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.909963</td>\n",
       "      <td>12.511600</td>\n",
       "      <td>13.311200</td>\n",
       "      <td>19.760000</td>\n",
       "      <td>11.343800</td>\n",
       "      <td>13.515900</td>\n",
       "      <td>14.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.842729</td>\n",
       "      <td>14.125900</td>\n",
       "      <td>14.606800</td>\n",
       "      <td>20.602800</td>\n",
       "      <td>11.446600</td>\n",
       "      <td>15.102700</td>\n",
       "      <td>15.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.194600</td>\n",
       "      <td>2.801257</td>\n",
       "      <td>14.274400</td>\n",
       "      <td>14.879900</td>\n",
       "      <td>22.844100</td>\n",
       "      <td>12.080200</td>\n",
       "      <td>15.398400</td>\n",
       "      <td>15.895400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.194600</td>\n",
       "      <td>2.767067</td>\n",
       "      <td>14.773600</td>\n",
       "      <td>16.014500</td>\n",
       "      <td>22.176200</td>\n",
       "      <td>12.530000</td>\n",
       "      <td>16.469300</td>\n",
       "      <td>16.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.194600</td>\n",
       "      <td>2.749725</td>\n",
       "      <td>15.893800</td>\n",
       "      <td>16.491500</td>\n",
       "      <td>22.898900</td>\n",
       "      <td>12.296300</td>\n",
       "      <td>17.381300</td>\n",
       "      <td>16.992300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.194600</td>\n",
       "      <td>2.716720</td>\n",
       "      <td>16.091100</td>\n",
       "      <td>17.279800</td>\n",
       "      <td>23.341900</td>\n",
       "      <td>12.672800</td>\n",
       "      <td>16.951600</td>\n",
       "      <td>17.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.194600</td>\n",
       "      <td>2.713619</td>\n",
       "      <td>16.347600</td>\n",
       "      <td>17.264500</td>\n",
       "      <td>23.451200</td>\n",
       "      <td>13.297000</td>\n",
       "      <td>17.556700</td>\n",
       "      <td>17.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.622800</td>\n",
       "      <td>2.702202</td>\n",
       "      <td>15.764900</td>\n",
       "      <td>17.667600</td>\n",
       "      <td>24.057400</td>\n",
       "      <td>13.183900</td>\n",
       "      <td>17.665700</td>\n",
       "      <td>17.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.622800</td>\n",
       "      <td>2.701033</td>\n",
       "      <td>16.249000</td>\n",
       "      <td>17.385000</td>\n",
       "      <td>25.079900</td>\n",
       "      <td>12.812100</td>\n",
       "      <td>17.657000</td>\n",
       "      <td>17.836600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.622800</td>\n",
       "      <td>2.687780</td>\n",
       "      <td>16.063800</td>\n",
       "      <td>17.973900</td>\n",
       "      <td>23.941700</td>\n",
       "      <td>13.836900</td>\n",
       "      <td>18.663100</td>\n",
       "      <td>18.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.622800</td>\n",
       "      <td>2.686927</td>\n",
       "      <td>16.359100</td>\n",
       "      <td>18.036200</td>\n",
       "      <td>24.872400</td>\n",
       "      <td>13.767000</td>\n",
       "      <td>18.789700</td>\n",
       "      <td>18.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.622800</td>\n",
       "      <td>2.673446</td>\n",
       "      <td>16.104600</td>\n",
       "      <td>18.827200</td>\n",
       "      <td>24.225300</td>\n",
       "      <td>14.420500</td>\n",
       "      <td>18.950600</td>\n",
       "      <td>18.505700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.437300</td>\n",
       "      <td>2.689181</td>\n",
       "      <td>16.924700</td>\n",
       "      <td>18.382100</td>\n",
       "      <td>24.383100</td>\n",
       "      <td>14.301200</td>\n",
       "      <td>18.240300</td>\n",
       "      <td>18.446300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.437300</td>\n",
       "      <td>2.678998</td>\n",
       "      <td>16.587300</td>\n",
       "      <td>17.988900</td>\n",
       "      <td>24.595600</td>\n",
       "      <td>14.351600</td>\n",
       "      <td>18.818300</td>\n",
       "      <td>18.468300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.437300</td>\n",
       "      <td>2.689528</td>\n",
       "      <td>16.862300</td>\n",
       "      <td>18.537900</td>\n",
       "      <td>25.394200</td>\n",
       "      <td>14.733900</td>\n",
       "      <td>19.640400</td>\n",
       "      <td>19.033700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-100\n",
      "Configuration saved in output-en-mul/checkpoint-100/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-200\n",
      "Configuration saved in output-en-mul/checkpoint-200/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-300\n",
      "Configuration saved in output-en-mul/checkpoint-300/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-1900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-400\n",
      "Configuration saved in output-en-mul/checkpoint-400/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-500\n",
      "Configuration saved in output-en-mul/checkpoint-500/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-600\n",
      "Configuration saved in output-en-mul/checkpoint-600/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-700\n",
      "Configuration saved in output-en-mul/checkpoint-700/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-800\n",
      "Configuration saved in output-en-mul/checkpoint-800/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-900\n",
      "Configuration saved in output-en-mul/checkpoint-900/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-1000\n",
      "Configuration saved in output-en-mul/checkpoint-1000/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-1100\n",
      "Configuration saved in output-en-mul/checkpoint-1100/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-1100/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-1100/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-1100/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-1200\n",
      "Configuration saved in output-en-mul/checkpoint-1200/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-1300\n",
      "Configuration saved in output-en-mul/checkpoint-1300/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-1300/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-1300/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-1300/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-1400\n",
      "Configuration saved in output-en-mul/checkpoint-1400/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-1500\n",
      "Configuration saved in output-en-mul/checkpoint-1500/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-1600\n",
      "Configuration saved in output-en-mul/checkpoint-1600/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to output-en-mul/checkpoint-1700\n",
      "Configuration saved in output-en-mul/checkpoint-1700/config.json\n",
      "Model weights saved in output-en-mul/checkpoint-1700/pytorch_model.bin\n",
      "tokenizer config file saved in output-en-mul/checkpoint-1700/tokenizer_config.json\n",
      "Special tokens file saved in output-en-mul/checkpoint-1700/special_tokens_map.json\n",
      "Deleting older checkpoint [output-en-mul/checkpoint-1500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from output-en-mul/checkpoint-1400 (score: 2.6734464168548584).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1700, training_loss=2.703928653492647, metrics={'train_runtime': 11309.4871, 'train_samples_per_second': 374.77, 'train_steps_per_second': 0.364, 'total_flos': 4.546194863652864e+16, 'train_loss': 2.703928653492647, 'epoch': 8.25})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca73daa",
   "metadata": {},
   "source": [
    "## Test translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f00e35",
   "metadata": {},
   "source": [
    "To use a different model version, checkpoints can be loaded as follows. \n",
    "\n",
    "Saved models are available in [this Drive folder](https://drive.google.com/drive/folders/1XJBKEPwwwwaSOoFdT4SL4EcFN6iPNPcg?usp=sharing), named according to the random `wandb.ai` title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58e3fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"savedmodels/en-mul-ethereal-valley\"\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5915c0-76d8-466d-a908-ea0cfba1ed46",
   "metadata": {},
   "source": [
    "Example of using an `en-mul` model. Note that we have to do the same pre-processing as was used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa9903e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(source_text, model, tokenizer):\n",
    "    device = torch.device('cpu')\n",
    "    model = model.eval()\n",
    "    model = model.to(device) \n",
    "    inputs = tokenizer(source_text, return_tensors=\"pt\").to(device)\n",
    "    tokens = model.generate(**inputs)\n",
    "    result = tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "def translate_to_multiple(query,\n",
    "                          languages = {'ach': 'Acholi', 'lgg': 'Lugbara', 'lug': 'Luganda','nyn': 'Runyankore', 'teo': 'Ateso'}):\n",
    "    normalizer = sacremoses.MosesPunctNormalizer()\n",
    "    normalized_query = sentence_format(normalizer.normalize(query))\n",
    "    \n",
    "    translations = []\n",
    "    for lang in ['lug', 'ach', 'nyn', 'teo', 'lgg']:\n",
    "        translation = translate(f\">>{lang}<< {normalized_query}\", model, tokenizer)\n",
    "        translations.append({'target': languages[lang], 'translation': translation})\n",
    "\n",
    "    df = pd.DataFrame(translations)\n",
    "    df = df.set_index('target')   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05389deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Luganda</th>\n",
       "      <td>Mu Kampala mulimu abantu bameka?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acholi</th>\n",
       "      <td>Dano adi matye I Kampala?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runyankore</th>\n",
       "      <td>Omuri Kampala harimu abantu bangahi?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ateso</th>\n",
       "      <td>Itunga bo idi ejaas Kampala?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lugbara</th>\n",
       "      <td>Ba Kampalaa 'diyi ma kalafe si?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     translation\n",
       "target                                          \n",
       "Luganda         Mu Kampala mulimu abantu bameka?\n",
       "Acholi                 Dano adi matye I Kampala?\n",
       "Runyankore  Omuri Kampala harimu abantu bangahi?\n",
       "Ateso               Itunga bo idi ejaas Kampala?\n",
       "Lugbara          Ba Kampalaa 'diyi ma kalafe si?"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"how many people are there in Kampala?\"\n",
    "\n",
    "translate_to_multiple(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08e49c-6c9d-4717-b4b5-b696896c2159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
