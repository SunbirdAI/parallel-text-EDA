{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SunbirdAI/parallel-text-EDA/blob/main/Prepare_supplementary_translation_data_(MT560%2BFLORES101%2BAI4D).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A4zDnR45o1gf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from utils.xml_parser import extract_alignments, extract_sentences, align_sentences, output_jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQF4tEMh5_5u"
   },
   "source": [
    "# Prepare Mozilla-110 data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    !mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/mozilla’: File exists\n",
      "Archive:  data/mozilla/ach.zip\n",
      "replace data/mozilla/ach/INFO? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "Archive:  data/mozilla/en.zip\n",
      "replace data/mozilla/en/INFO? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "^C\n",
      "gzip: data/mozilla/ach-en.xml already exists;\tnot overwritten\n",
      "yes: standard output: Broken pipe\n",
      "gzip: data/mozilla/en-lg.xml already exists;\tnot overwritten\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/mozilla\n",
    "if not os.path.exists(\"data/mozilla/ach-en.xml\"):\n",
    "    !wget -O data/mozilla/ach-en.xml.gz https://object.pouta.csc.fi/OPUS-Mozilla-I10n/v1/xml/ach-en.xml.gz\n",
    "if not os.path.exists(\"data/mozilla/en-lg.xml\"):\n",
    "    !wget -O data/mozilla/en-lg.xml.gz https://object.pouta.csc.fi/OPUS-Mozilla-I10n/v1/xml/en-lg.xml.gz\n",
    "if not os.path.exists(\"data/mozilla/ach\"):        \n",
    "    !wget -O data/mozilla/ach.zip https://object.pouta.csc.fi/OPUS-Mozilla-I10n/v1/raw/ach.zip\n",
    "if not os.path.exists(\"data/mozilla/en\"):\n",
    "    !wget -O data/mozilla/en.zip https://object.pouta.csc.fi/OPUS-Mozilla-I10n/v1/raw/en.zip\n",
    "if not os.path.exists(\"data/mozilla/lg\"):\n",
    "    !wget -O data/mozilla/lg.zip https://object.pouta.csc.fi/OPUS-Mozilla-I10n/v1/raw/lg.zip\n",
    "\n",
    "!unzip data/mozilla/ach.zip -d data/mozilla/ach\n",
    "!unzip data/mozilla/en.zip -d data/mozilla/en\n",
    "!unzip data/mozilla/lg.zip -d data/mozilla/lg\n",
    "!yes y | gunzip data/mozilla/ach-en.xml.gz \n",
    "!yes y | gunzip data/mozilla/en-lg.xml.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGy0jDQy5GWy",
    "outputId": "75157706-881f-4f0b-d81d-49b72aa0043b"
   },
   "outputs": [],
   "source": [
    "path_to_ach = \"data/mozilla/ach/Mozilla-I10n/raw/ach/mozilla-i10n-v1.xml\"\n",
    "path_to_eng = \"data/mozilla/en/Mozilla-I10n/raw/en/mozilla-i10n-v1.xml\"\n",
    "path_to_lug = \"data/mozilla/lg/Mozilla-I10n/raw/lg/mozilla-i10n-v1.xml\"\n",
    "alignment_ach = \"data/mozilla/ach-en.xml\"\n",
    "alignment_lug = \"data/mozilla/en-lg.xml\"\n",
    "            \n",
    "en_ach_alignments = extract_alignments(alignment_ach, eng_first = False)\n",
    "en_lug_alignments = extract_alignments(alignment_lug, eng_first = True)\n",
    "\n",
    "ach_sentences = extract_sentences(path_to_ach)\n",
    "eng_sentences = extract_sentences(path_to_eng)\n",
    "lug_sentences = extract_sentences(path_to_lug)\n",
    "\n",
    "accumulator_dict = dict()\n",
    "accumulator_dict = align_sentences(accumulator_dict, eng_sentences, ach_sentences, en_ach_alignments, \"ach\")\n",
    "accumulator_dict = align_sentences(accumulator_dict, eng_sentences, lug_sentences, en_lug_alignments, \"lug\")\n",
    "\n",
    "output_jsonl(accumulator_dict, \"mozilla_110.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIGCHMO6-VRk"
   },
   "source": [
    "# Prepare TICO19 data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TFzQb2jL5mN6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-30 16:41:30--  https://object.pouta.csc.fi/OPUS-tico-19/v2020-10-28/xml/en-lg.xml.gz\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.19|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 24221 (24K) [application/gzip]\n",
      "Saving to: ‘data/tico19/en-lg.xml.gz’\n",
      "\n",
      "data/tico19/en-lg.x 100%[===================>]  23.65K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-03-30 16:41:33 (168 KB/s) - ‘data/tico19/en-lg.xml.gz’ saved [24221/24221]\n",
      "\n",
      "yes: standard output: Broken pipe\n",
      "--2023-03-30 16:41:33--  https://object.pouta.csc.fi/OPUS-tico-19/v2020-10-28/raw/en.zip\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.19|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5999906 (5.7M) [application/zip]\n",
      "Saving to: ‘data/tico19/en.zip’\n",
      "\n",
      "data/tico19/en.zip  100%[===================>]   5.72M   200KB/s    in 28s     \n",
      "\n",
      "2023-03-30 16:42:02 (210 KB/s) - ‘data/tico19/en.zip’ saved [5999906/5999906]\n",
      "\n",
      "Archive:  data/tico19/en.zip\n",
      "  inflating: data/tico19/en/INFO     \n",
      "  inflating: data/tico19/en/README   \n",
      "  inflating: data/tico19/en/LICENSE  \n",
      "   creating: data/tico19/en/tico-19/\n",
      "   creating: data/tico19/en/tico-19/raw/\n",
      "   creating: data/tico19/en/tico-19/raw/en/\n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ti_ER.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-zh.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ta.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ku.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-din.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ckb.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ti_ET.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ln.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ps.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ru.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-bn.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-nus.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-fuv.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-tl.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ar.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-so.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-sw.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ms.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-rw.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-prs.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-es-LA.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-my.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ha.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ur.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-km.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-mr.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-lg.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-pt-BR.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-kr.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-fr.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-zu.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-id.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-ne.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-om.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-hi.xml  \n",
      "  inflating: data/tico19/en/tico-19/raw/en/all.en-fa.xml  \n",
      "--2023-03-30 16:42:02--  https://object.pouta.csc.fi/OPUS-tico-19/v2020-10-28/raw/lg.zip\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.19|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 366234 (358K) [application/zip]\n",
      "Saving to: ‘data/tico19/lg.zip’\n",
      "\n",
      "data/tico19/lg.zip  100%[===================>] 357.65K  79.0KB/s    in 4.5s    \n",
      "\n",
      "2023-03-30 16:42:08 (79.0 KB/s) - ‘data/tico19/lg.zip’ saved [366234/366234]\n",
      "\n",
      "Archive:  data/tico19/lg.zip\n",
      "  inflating: data/tico19/lg/INFO     \n",
      "  inflating: data/tico19/lg/README   \n",
      "  inflating: data/tico19/lg/LICENSE  \n",
      "   creating: data/tico19/lg/tico-19/\n",
      "   creating: data/tico19/lg/tico-19/raw/\n",
      "   creating: data/tico19/lg/tico-19/raw/lg/\n",
      "  inflating: data/tico19/lg/tico-19/raw/lg/all.fr-lg.xml  \n",
      "  inflating: data/tico19/lg/tico-19/raw/lg/all.en-lg.xml  \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"data/tico19\"):\n",
    "    !mkdir data/tico19\n",
    "if not os.path.exists(\"data/tico19/en-lg.xml\"):\n",
    "    !wget -O data/tico19/en-lg.xml.gz https://object.pouta.csc.fi/OPUS-tico-19/v2020-10-28/xml/en-lg.xml.gz\n",
    "    !yes y | gunzip data/tico19/en-lg.xml.gz\n",
    "if not os.path.exists(\"data/tico19/en\"):\n",
    "    !wget -O data/tico19/en.zip https://object.pouta.csc.fi/OPUS-tico-19/v2020-10-28/raw/en.zip\n",
    "    !unzip data/tico19/en.zip -d data/tico19/en\n",
    "if not os.path.exists(\"data/tico19/lg\"):\n",
    "    !wget -O data/tico19/lg.zip https://object.pouta.csc.fi/OPUS-tico-19/v2020-10-28/raw/lg.zip\n",
    "    !unzip data/tico19/lg.zip -d data/tico19/lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Notice I manually go through the path_to_eng and path_to_eng files and remove any <p> and </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SQ-EIiMgs5as"
   },
   "outputs": [],
   "source": [
    "path_to_eng = \"data/tico19/en/tico-19/raw/en/all.en-lg.xml\"\n",
    "path_to_lug = \"data/tico19/lg/tico-19/raw/lg/all.en-lg.xml\"\n",
    "alignment_lug = \"data/tico19/en-lg.xml\"\n",
    "            \n",
    "en_lug_alignments = extract_alignments(alignment_lug, eng_first = True)\n",
    "\n",
    "eng_sentences = extract_sentences(path_to_eng)\n",
    "lug_sentences = extract_sentences(path_to_lug)\n",
    "\n",
    "accumulator_dict = dict()\n",
    "accumulator_dict = align_sentences(accumulator_dict, eng_sentences, lug_sentences, en_lug_alignments, \"lug\")\n",
    "\n",
    "output_jsonl(accumulator_dict, \"tico19.jsonl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAFAND-MT\n",
    "\n",
    "This dataset contains sentences with translations in 21 different African languages in the news domain. We combine the train, dev and devtest splits into a single set. Or leave a validation set for languages which are not found in salt to give an estimate of performance.\n",
    "\n",
    "The current git repo is https://github.com/masakhane-io/lafand-mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'automlzero' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/ali/.pyenv/versions/automlzero/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"lafand-mt\"):\n",
    "    !git clone https://github.com/masakhane-io/lafand-mt\n",
    "\n",
    "LAFAND_MT_PATH = \"lafand-mt/data/json_files\"\n",
    "#Specify which pairs to use\n",
    "LAFAND_PAIRS = [\"en-lug\", \"en-luo\"] \n",
    "\n",
    "COMBINE_TRAIN_TEST = True\n",
    "\n",
    "\n",
    "for pair in LAFAND_PAIRS:\n",
    "    read_src_lang, read_tgt_lang = pair.split(\"-\")\n",
    "\n",
    "    read_pair_path = os.path.join(LAFAND_MT_PATH, pair) \n",
    "    output_file = f\"lafand-{read_src_lang}-{read_tgt_lang}-combined.jsonl\"\n",
    "    \n",
    "    # We use 3 letter codes in our data\n",
    "    if read_src_lang == \"en\":\n",
    "        write_src_lang = \"eng\"\n",
    "    else:\n",
    "        write_src_lang = read_src_lang\n",
    "\n",
    "    if read_tgt_lang == \"en\":\n",
    "        write_tgt_lang = \"eng\"\n",
    "    else:\n",
    "        write_tgt_lang = read_tgt_lang\n",
    "\n",
    "    if COMBINE_TRAIN_TEST:\n",
    "\n",
    "        with open(OUTPUT_DIR + output_file, \"w\") as outfile, \\\n",
    "            open( os.path.join(read_pair_path, \"train.json\"), \"r\" ) as trfd, \\\n",
    "            open( os.path.join(read_pair_path, \"dev.json\"), \"r\" ) as dfd, \\\n",
    "            open( os.path.join(read_pair_path, \"test.json\"), \"r\" ) as tefd:\n",
    "\n",
    "            for line in trfd.readlines():\n",
    "                line = json.loads(line)\n",
    "\n",
    "                outdict = {\n",
    "                    \"text\": {\n",
    "                    write_src_lang: line[\"translation\"][read_src_lang],\n",
    "                    write_tgt_lang: line[\"translation\"][read_tgt_lang]\n",
    "                    }\n",
    "                }\n",
    "                json.dump(outdict, outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "            for line in dfd.readlines():\n",
    "                line = json.loads(line)\n",
    "\n",
    "                outdict = {\n",
    "                    \"text\": {\n",
    "                    write_src_lang: line[\"translation\"][read_src_lang],\n",
    "                    write_tgt_lang: line[\"translation\"][read_tgt_lang]\n",
    "                    }\n",
    "                }\n",
    "                json.dump(outdict, outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "                \n",
    "            for line in tefd.readlines():\n",
    "                line = json.loads(line)\n",
    "\n",
    "                outdict = {\n",
    "                    \"text\": {\n",
    "                    write_src_lang: line[\"translation\"][read_src_lang],\n",
    "                    write_tgt_lang: line[\"translation\"][read_tgt_lang]\n",
    "                    }\n",
    "                }\n",
    "                json.dump(outdict, outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "                \n",
    "            \n",
    "\n",
    "    else:\n",
    "        train_output_path = \"train_\" + output_file\n",
    "        valid_output_path = \"valid_\" + output_file\n",
    "        test_output_path = \"test_\" + output_file\n",
    "        with open(OUTPUT_DIR + train_output_path, \"w\") as train_outfile, \\\n",
    "                open(OUTPUT_DIR + valid_output_path, \"w\") as valid_outfile, \\\n",
    "                open(OUTPUT_DIR + test_output_path, \"w\") as test_outfile, \\\n",
    "                open( os.path.join(read_pair_path, \"train.json\"), \"r\" ) as trfd, \\\n",
    "                open( os.path.join(read_pair_path, \"dev.json\"), \"r\" ) as dfd, \\\n",
    "                open( os.path.join(read_pair_path, \"test.json\"), \"r\" ) as tefd:\n",
    "\n",
    "            for line in trfd.readlines():\n",
    "                line = json.loads(line)\n",
    "\n",
    "                outdict = {\n",
    "                    \"text\": {\n",
    "                    write_src_lang: line[\"translation\"][read_src_lang],\n",
    "                    write_tgt_lang: line[\"translation\"][read_tgt_lang]\n",
    "                    }\n",
    "                }\n",
    "                json.dump(outdict, train_outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "            for line in dfd.readlines():\n",
    "                line = json.loads(line)\n",
    "\n",
    "                outdict = {\n",
    "                    \"text\": {\n",
    "                    write_src_lang: line[\"translation\"][read_src_lang],\n",
    "                    write_tgt_lang: line[\"translation\"][read_tgt_lang]\n",
    "                    }\n",
    "                }\n",
    "                json.dump(outdict, valid_outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "                \n",
    "            for line in tefd.readlines():\n",
    "                line = json.loads(line)\n",
    "\n",
    "                outdict = {\n",
    "                    \"text\": {\n",
    "                    write_src_lang: line[\"translation\"][read_src_lang],\n",
    "                    write_tgt_lang: line[\"translation\"][read_tgt_lang]\n",
    "                    }\n",
    "                }\n",
    "                json.dump(outdict, test_outfile)\n",
    "                outfile.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMwvnwQD9IQgg+UrHXLMZW8",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e56a3f7d6087dc0e8010c68576613beaec95fa9bfb8de85e967e8c762a16959e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
