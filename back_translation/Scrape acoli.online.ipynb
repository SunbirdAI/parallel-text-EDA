{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29eefa67-1fe8-49d3-8574-f080d4d569ac",
   "metadata": {},
   "source": [
    "[acoli.online](acoli.online) is a site with folktalks, poetry and other resources in the Acholi language. It is a static site so doesn't need to be repeatedly scraped. This notebook yields about 2000 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "115463d1-13e9-4d3d-a414-bbab38c65198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   \n",
    "import urllib\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9f6b4edb-2a7d-439c-8b41-716aff4e89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "465884e4-2ec2-4859-875a-2e8ddd7c947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "ododo_pages = [1170, 1188, 1203, 1023, 640, 637, 626, 850,\n",
    "         630, 623, 608, 532, 528, 524, 520, 516] # ododo\n",
    "\n",
    "for page_id in ododo_pages:\n",
    "    url = \"https://acoli.online/?page_id=1203\"    \n",
    "    raw_html = urllib.request.urlopen(url).read()    \n",
    "    \n",
    "    html = (str(raw_html).replace('<br>', r'\\n'))\n",
    "    html = html.replace(r'\\t', '\\n').replace(r'\\r', '\\n').replace(r'\\n', '\\n')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = str(soup.get_text())\n",
    "    lines = [line for line in text.split('\\n') if len(line)>5]\n",
    "    lines = lines[8:-2]\n",
    "    sentences.extend(split_into_sentences(lines[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ba823e05-9809-4ac2-bb58-b6214adf00fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry\n",
    "poetry_pages = [346, 365, 392, 395, 397, 399]\n",
    "\n",
    "for page_id in poetry_pages:\n",
    "    url = \"https://acoli.online/?page_id=346\"    \n",
    "    raw_html = urllib.request.urlopen(url).read()  \n",
    "\n",
    "    html = (str(raw_html).replace('<br>', '\\n'))\n",
    "    html = html.replace(r'\\t', '\\n').replace(r'\\r', '\\n').replace(r'\\n', '\\n')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = str(soup.get_text())\n",
    "    lines = [line for line in text.split('\\n') if len(line)>5]\n",
    "    lines = lines[9:-2]\n",
    "    sentences.extend(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8e874c1e-d13a-45e2-8390-07358e309ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"acholi-online.txt\", \"w\") as f:\n",
    "    f.writelines('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3ee1c-1196-4fed-a5f9-d6655aaea05d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
