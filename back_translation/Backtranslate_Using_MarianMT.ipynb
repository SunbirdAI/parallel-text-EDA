{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK8sWFAPyJkF"
      },
      "source": [
        "#### We can use our best trained models to generate high quality sentences to be added to the SALT training corpus increasing the performance and quality of our models.\n",
        "\n",
        "Backtranslation is translation of monolingual text from the target sentences to monolingual text in the source sentences, usually using a pretrained translation model. This data is then fed into the model's training data and used to finetune the model. Forward translation utilizes mono-lingual data in the source language, which is usually less effective, since the same model is used to translate the data and retrain on it.\n",
        "\n",
        "In this case we use our pretrained English to Many and Many to English models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "31RZZhkMyJkM"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import sacremoses\n",
        "import transformers\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4hBmE6O8yJkP"
      },
      "outputs": [],
      "source": [
        "#find the necessary ggl language codes here https://cloud.google.com/translate/docs/languages\n",
        "config = {\n",
        "    \"bt_src_langs\": [\"ach\",\"lgg\", \"lug\",\"nyn\",\"teo\"],\n",
        "    \"bt_tgt_langs\": [\"eng\"],\n",
        "    \"model_name\": \"BT_14_3_m2en\",\n",
        "    \"model_checkpoint\": \"/content/drive/MyDrive/john_models/saved_models/marianmt-many-eng/marianmt-many-eng-checkpoint-700/\",\n",
        "    'max_input_length': 128,\n",
        "\n",
        "    \"monolingual_data_dir\": \"parallel-text-EDA/back_translation/data\",\n",
        "}\n",
        "\n",
        "config[\"output_data_dir\"] = \"/content/drive/MyDrive/BT/MarianMT_M2E_14_3_23\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJed_h8lyJkR",
        "outputId": "ac133825-80cd-4a3b-8db3-441c66b848a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bukedde.txt\n",
            "makerere.txt\n",
            "misc.txt\n",
            "acholi-online.txt\n",
            "rupiny.txt\n"
          ]
        }
      ],
      "source": [
        "## Todo: Add config dict\n",
        "sentences = {language:[] for language in config[\"bt_src_langs\"]}\n",
        "languages_to_bt = os.listdir(config[\"monolingual_data_dir\"])\n",
        "for bt_language in languages_to_bt:\n",
        "    if bt_language in config[\"bt_src_langs\"]:\n",
        "        language_dir = os.path.join(config[\"monolingual_data_dir\"], bt_language)\n",
        "        files_to_read = os.listdir(language_dir)\n",
        "        for file_to_read in files_to_read:\n",
        "            if file_to_read.startswith(\".\"):\n",
        "                continue\n",
        "            with open(os.path.join(language_dir,file_to_read)) as lfd: \n",
        "                print(file_to_read)\n",
        "                lines = lfd.readlines()\n",
        "            sentences[bt_language].extend(lines)\n",
        "\n",
        "lug_data_raw = datasets.Dataset.from_dict({\"lug\":sentences[\"lug\"]})\n",
        "ach_data_raw = datasets.Dataset.from_dict({\"ach\":sentences[\"ach\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-EzDnpdbIKj",
        "outputId": "e5f1cc2e-2da4-43f7-fec1-c9b82117baf3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['lug'],\n",
              "    num_rows: 12304\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lug_data_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh6z7Cx7Kf84",
        "outputId": "c621001d-e39a-49a5-fa31-20655e297e86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-86711c19efc8>:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = datasets.load_metric('sacrebleu')\n"
          ]
        }
      ],
      "source": [
        "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(config['model_checkpoint'])\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(config['model_checkpoint'])\n",
        "data_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model = model) \n",
        "metric = datasets.load_metric('sacrebleu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh-QToR1dHtd",
        "outputId": "6084d776-f233-4f47-cdc4-b3a2e8dfccd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [13:53<00:00,  8.59s/it]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "lug_results = []\n",
        "for text_idx in tqdm(range(0,len(lug_data_raw), 128)):\n",
        "\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "    try:\n",
        "        encoded = tokenizer(\n",
        "            lug_data_raw[text_idx:text_idx+128][\"lug\"],padding=True, return_tensors=\"pt\").to(device)\n",
        "    except IndexError:\n",
        "        encoded = tokenizer(\n",
        "            lug_data_raw[text_idx:][\"lug\"],padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_tokens = model.to(device).generate(**encoded, max_length=128)\n",
        "    result = tokenizer.batch_decode(\n",
        "        generated_tokens.cpu(), skip_special_tokens=True)\n",
        "    lug_results.extend(result)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2OXcwykFxt7_"
      },
      "outputs": [],
      "source": [
        "language_dir = os.path.join(config[\"output_data_dir\"], \"lug\" )\n",
        "if not os.path.exists(language_dir):\n",
        "    os.mkdir(language_dir)\n",
        "\n",
        "assert len(lug_results) == len(lug_data_raw)\n",
        "\n",
        "filepath = os.path.join(language_dir, \"bt_lug_en.jsonl\")\n",
        "with open(filepath, \"w\" ) as outfile:\n",
        "\n",
        "    for idx in range(len(lug_results)):\n",
        "        dict_to_dump = {\n",
        "        \"text\": {\n",
        "            \"lug\": lug_data_raw[idx][\"lug\"],\n",
        "            \"eng\": lug_results[idx]\n",
        "            }\n",
        "        }\n",
        "        json.dump(dict_to_dump, outfile)\n",
        "        outfile.write('\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO4ImfY4pVOy",
        "outputId": "1f4e02aa-e574-4299-e1b7-9f00505ac019"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [07:03<00:00,  8.14s/it]\n"
          ]
        }
      ],
      "source": [
        "ach_results = []\n",
        "for text_idx in tqdm(range(0,len(ach_data_raw), 128)):\n",
        "\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "    try:\n",
        "        encoded = tokenizer(\n",
        "            ach_data_raw[text_idx:text_idx+128][\"ach\"],padding=True, return_tensors=\"pt\").to(device)\n",
        "    except IndexError:\n",
        "        encoded = tokenizer(\n",
        "            ach_data_raw[text_idx:][\"ach\"],padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_tokens = model.to(device).generate(**encoded, max_length=128)\n",
        "    result = tokenizer.batch_decode(\n",
        "        generated_tokens.cpu(), skip_special_tokens=True)\n",
        "    ach_results.extend(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lXdfq2Cexsf3"
      },
      "outputs": [],
      "source": [
        "language_dir = os.path.join(config[\"output_data_dir\"], \"ach\" )\n",
        "if not os.path.exists(language_dir):\n",
        "    os.mkdir(language_dir)\n",
        "\n",
        "assert len(ach_results) == len(ach_data_raw)\n",
        "\n",
        "filepath = os.path.join(language_dir, \"bt_ach_en.jsonl\")\n",
        "with open(filepath, \"w\" ) as outfile:\n",
        "\n",
        "    for idx in range(len(ach_results)):\n",
        "        dict_to_dump = {\n",
        "        \"text\": {\n",
        "            \"ach\": ach_data_raw[idx][\"ach\"],\n",
        "            \"eng\": ach_results[idx]\n",
        "            }\n",
        "        }\n",
        "        json.dump(dict_to_dump, outfile)\n",
        "        outfile.write('\\n')\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "automlzero",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e56a3f7d6087dc0e8010c68576613beaec95fa9bfb8de85e967e8c762a16959e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
